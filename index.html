<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="images/smile.png">
  <title>Yan Hao</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="65%" valign="middle">
              <p align="center">
                <name>Yan Hao</name>
              </p>
              <p>I'm Yan Hao, a computer science master student at ETH Zürich. Before that, I received the B.S. degree at <a href="https://acm.sjtu.edu.cn/home"> ACM Honors Class, Zhiyuan College</a>, Shanghai Jiao Tong University (SJTU). 
                <!-- I am a visiting student at UC Berkeley, advised by Prof. <a href="https://www.yf.io/">Fisher Yu</a> and Prof. <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>. Before that, I did research in 3D vision, advised by Professor Cewu Lu, <a href="http://mvig.sjtu.edu.cn/">MVIG</a>. -->
              <p>

                I am interested in machine learning and computer vision, especially 3D vision. At ETH, I contributed to a 3D Vision project supervised by <a href="https://ir0.github.io/">Dr. Iro Armeni</a>. My master thesis was done at EPFL under the supervision of <a href="https://florentfo.rest/">Dr. Florent Forest</a> and <a href="https://people.epfl.ch/olga.fink?lang=en">Prof. Dr. Olga Fink</a>. 
              </p>

              <p align=center>
                <a href="mailto:hyhaoyan98@gmail.com">Email</a> &nbsp/&nbsp
<!--                 <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp -->
<!--                 <a href="data/JonBarron-bio.txt">Biography</a> &nbsp/&nbsp -->
                <a href="https://github.com/honeyhaoyan">Github</a> &nbsp/&nbsp

                <a href=./Yan_Hao_CV_2023.pdf>CV</a> 
<!--                 <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Google Scholar</a> &nbsp/&nbsp -->
<!--                 <a href="http://www.linkedin.com/in/jonathanbarron/"> LinkedIn </a> -->
              </p>
            </td>
            <td width="35%">
              <img width="90%" src="images/photo.JPG">
            </td>
          </tr>
        </table>

        <!-- news -->
        <!-- table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>News</heading>
              <p>
                2019.11: Two papers submitted to CVPR 2020.
              </p>
              <p>
                2019.07: Join Berkeley DeepDrive & BAIR lab as a visiting student!
              </p>
            </td>
          </tr>
        </table -->

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Education</heading>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <td valign="top" width="75%">
            <font size=3>Master Thesis, <b>EPFL, Switzerland</b></font>
            <br>
            <!-- DeepDrive & BAIR lab, supervised by Prof. <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a> and Prof. <a href="https://www.yf.io/">Fisher Yu</a>. -->
            <!-- <br> -->
            <em>Nov. 2022 - June. 2023</em>
          </td>
          <td width="15%">
            <img width=100% src='images/EPFL.png'>
          </td>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <td valign="top" width="75%">
            <font size=3>Master in Computer Science, <b>ETH Zürich, Switzerland</b></font>
            <br>
            <!-- DeepDrive & BAIR lab, supervised by Prof. <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a> and Prof. <a href="https://www.yf.io/">Fisher Yu</a>. -->
            <!-- <br> -->
            <em>Sep. 2020 - Present</em>
          </td>
          <td width="15%">
            <img width=100% src='images/eth.svg.png'>
          </td>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <td valign="top" width="75%">
            <font size=3>B.S. in Computer Science, <b>Shanghai Jiao Tong University</b></font>
            <br>

            <a href="https://acm.sjtu.edu.cn/home">ACM Honors Class</a>, supervised by <a href="http://apex.sjtu.edu.cn/members/yyu">Prof. Yong Yu</a>.
            <br>
            <em>Sept. 2016 to Jun. 2020</em>
          </td>
          <td width="16%">
            <img width=100% src='images/sjtu.png'>
          </td>
        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Internship</heading>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <td valign="top" width="75%">
            <font size=3>Research Intern, <b>Amazon AWA Shanghai AI Lab</b></font>
            <br>
            Supervised by Tong He and Tianjun Xiao</a>.
            <br>
            <em>Jun. 2020 to Sept. 2020</em>
          </td>
          <td width="15%">
            <img width=100% src='images/Amazon-logo.png'>
          </td>
        </table>



        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research</heading>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr onmouseout="hake_stop()" onmouseover="hake_start()">
            <td width="25%">
              <img width=100% src='images/thesis.png'>
            </td>
            <td valign="middle" width="75%">
                <papertitle>Source Free Domain Adaptation for Object Detection applied to Road Scene Understanding</papertitle>
              <br><strong>Yan Hao*</strong>, Florent Forest*, Olga Fink
              <br>
              We propose a new source-free domain adaptation method for object detection applied to road scene understanding. Implementation bases on Detectron2 and Pytorch. Achieve state-of-the-art results for the adaptation from Cityscapes to Cityscapes Foggy and from Sim10k to Cityscapes.
              <br>
              <em>Master Thesis. In preparation for conference submission.</em>
              <br>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr onmouseout="hake_stop()" onmouseover="hake_start()">
            <td width="25%">
              <img width=100% src='images/NSS.png'>
            </td>
            <td valign="middle" width="75%">
              <a href="http://nothing-stands-still.com/">
                <papertitle>Nothing Stands Still: A Spatiotemporal Benchmark on 3D Point Cloud Registration Under Large Geometric and Temporal Change </papertitle>
              </a>
              <br>Tao Sun, <strong>Yan Hao</strong>, Shengyu Huang, Silvio Savarese, Konrad Schindler, Marc Pollefeys, Iro Armeni
              <br>
              We propose a new spatiotemporal dataset and benchmark called NSS (Nothing Stands Still) on 3D point
cloud registration under large geometric change across temporal stages. Accepted at CVPR 2023 DEMO.
              <br>
              <em>In submission.</em>
              <br>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr onmouseout="hake_stop()" onmouseover="hake_start()">
            <td width="25%">
              <img width=100% src='images/objectness.png'>
            </td>
            <td valign="middle" width="75%">
              <a href="https://arxiv.org/abs/1912.02332">
                <papertitle>3D Objectness Estimation via Bottom-up Regret Grouping</papertitle>
              </a>
              <br>Zelin Ye, <strong>Yan Hao</strong>, Liang Xu, Rui Zhu, 
              Cewu Lu
              <br>
              We propose a robust 3D objectness estimation method in a bottom-up manner, i.e. first over-segment scene pointclouds  and  then  group  them  iteratively  with  a  novel  regret  mechanism  to  withdraw  incorrect  groupings.
              <br>
              <em>Arvix.</em>
              <br>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr onmouseout="hake_stop()" onmouseover="hake_start()">
            <td width="25%">
              <img width=100% src='images/pal_net.png'>  
            </td>
            <td valign="middle" width="75%">
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9428175">
                <papertitle>PAL-Net: Predicate-Aware Learning for Scene Graph Generation</papertitle>
              </a>
              <br>Liang Xu, Yong-Lu Li, Minyang Chen, <strong>Yan Hao</strong>, 
              Cewu Lu
              <br>
              Our proposed PAL-Net has two ingredients for scene graph generation.  First we introduce a novel embedding loss for translation embedding in a metric learning manner. Then we take predicates as conditions for contextualmodeling to alleviate noise.
              <br>
              <em>ICME. Oral.</em>
              <br>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr onmouseout="unprocessing_stop()" onmouseover="unprocessing_start()">
            <td width="25%">
              <img width=100% src='images/mva.jpg'>
            </td>
            <td valign="middle" width="75%">
              <a href="http://www.mva-org.jp/Proceedings/2019/papers/05-20.pdf">
                <papertitle>Visual Rhythm Prediction with Feature-Aligned Network</papertitle>
              </a>
              <br>Yutong Xie, Haiyang Wang, <strong>Yan Hao</strong>, Zihao Xu
              <br>
              The paper proposed a data-driven visual rhythm prediction method, in which several visual features are extractedand then fed into an end-to-end neural network to predict the visual onsets.
              <br>
              <em>MVA</em> &nbsp
              <br>
            </td>
          </tr>
        </table>

        <!-- <br><br><br> -->
        <!-- <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=c0c0c0&w=350&t=tt&d=EJG7xYMEBwtNsq0qbwK5FjnjQTTc0vTFcQPreGILEBk&co=ffffff&ct=000000&cmo=3acc3a&cmn=ff5353'></script> -->

        <br><br>
        

      </td>
    </tr>
  </table>
</body>

</html>
